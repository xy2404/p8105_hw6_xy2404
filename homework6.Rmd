---
title: "Homework 6"
author: "Annie Yu xy2404"
date: "11/22/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(purrr)
library(ggplot2)
```

#Problem 1

####Cleaning the data and add variable....
```{r, message=FALSE, warning=FALSE}
homicide <- read_csv('./data/homicide-data.csv') %>% 
  janitor::clean_names() %>%
  mutate(city_state = str_c(city, ",", state)) %>% 
  filter(city_state!='Dallas,TX',
         city_state!='Phoenix,AZ',
         city_state!='Kansas City,MO',
         city_state!='Tulsa,AL') %>%
  mutate(resolved = as.numeric(disposition == "Closed by arrest"),
         victim_race = as.factor(victim_race),
         victim_age = as.numeric(victim_age)) %>% 
  mutate(victim_race = fct_recode(victim_race, non_white = 'Asian', non_white = 'Black', non_white = 'Hispanic', non_white = 'Other', non_white = 'Unknown', white = 'White')) %>% 
  mutate(victim_race = fct_relevel(victim_race,'white'))
```

First, I clean the dataset, Then I create new variable for city_state and make the data tidy. such as filter some variables I do not use. The dataset contains `r colnames(homicide)` variables. And the dataset is stored in the dataframe, which has `r nrow(homicide)` rows and `r ncol(homicide)` columns.


####For the city of Baltimore, MD....
```{r}
bal_homicide <- homicide %>% 
  filter(city_state == 'Baltimore,MD') %>% 
  select(resolved, victim_race, victim_age, victim_sex) 

  bal_glm <- glm(resolved ~ victim_age + victim_race + victim_sex, data = bal_homicide, family = binomial())
  

save(bal_glm, file = "./bal_homicide.rdata")
  
bal_glm %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         log_OR = estimate,
         OR_lower = exp(estimate - std.error*1.96),
         OR_upper = exp(estimate + std.error*1.96)) %>% 
  select(c(term, log_OR, OR, p.value, OR_lower, OR_upper)) %>% 
  knitr::kable()
  
```

For the city of Baltimore, MD, I want to know whether there are assoicaiton between resolved and other predictors (victim age, sex and race), I use the glm function to fit the logistic regression. After setting the glm function. I save the output as bal_homicide.rdata. Then apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed.


####Calculating for each city....
```{r}
glmfunction = function(x){
  
  glm_city <- glm(resolved ~ victim_age + victim_race + victim_sex, data = x, family = binomial())
  
  glm_city %>% 
    broom::tidy() %>% 
    mutate(OR = exp(estimate),
         log_OR = estimate,
         OR_lower = exp(estimate - std.error*1.96),
         OR_upper = exp(estimate + std.error*1.96)) %>% 
    select(c(term, log_OR, OR, p.value, OR_lower, OR_upper))
}

all_city <- nest(homicide, -city_state) %>% 
  mutate(adjust_or = map(data, glmfunction))%>% 
  select(city_state, adjust_or) %>% 
  unnest %>% 
  filter(term=='victim_racenon_white')
 
knitr::kable(all_city)
  
```

Creating a function to run glm for each of the cities in my dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims. The smaller the adjusted OR suggests more race discrimination in resolving homicides.


####Create a plot that shows the estimated ORs and CIs for each city
```{r, message=FALSE}
all_city %>% 
ggplot(aes(x = reorder(city_state, -OR), y=OR,fill = city_state))+
  geom_bar(stat = 'identity', alpha = 1)+
  geom_errorbar(mapping=aes(x= city_state, ymin=OR_lower, ymax=OR_upper))+
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1), legend.key.width = unit(0.15,'cm')) +
  labs(
    x = "City_State",
    y = "Adjusted Odds Ratio with 95% CIs"
  )
  ggtitle('Adjusted Odds Ratio with 95% CIs for each city')
  
```

I create the bar diagram to show the difference OR for each city. From this plot, we could conclude  Boston has least race discrimination while Tempa has the highest race discrimation. 


#Problem 2
```{r, message=FALSE}
birthweight<- read_csv('./data/birthweight.csv') %>% 
  janitor::clean_names() %>%
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

skimr::skim(birthweight)

# The full model is:
fullmodel <- lm(bwt ~ ., data = birthweight)
summary(fullmodel)

```

I clean the dataset firstly, Then I change the character of variables and  make the data tidy. The dataset contains `r colnames(birthweight)` variables. And the dataset is stored in the dataframe, which has `r nrow(birthweight)` rows and `r ncol(birthweight)` columns. I want to explore the association between bwt and other all predictors, I use the lm function to find it.


#### Hypothesis:
$$
Y =\beta_0 + \beta_1*(x_{1})babysex + \beta_2*(x_{2})bhead +\beta_3*(x_{3})blength+\beta_4*(x_{4})wtgain+ \beta_5*(x_{5})smoken
$$

#### Test the model:
```{r}
model1 <- lm(bwt ~ babysex + bhead + blength + wtgain +smoken, data = birthweight )
summary(model1)

birthweight %>% 
  add_predictions(model1) %>% 
  add_residuals(model1) %>%
  rename(prediction = pred,
         residuals = resid) %>% 
  ggplot(aes(x = prediction, y = residuals))+
  geom_point(aes(alpha=.2))+
  labs(
    x = "Prediction",
    y = "Residuals"
  )+
  ggtitle('Model Residuals Against Fitted Values')
  
  
```

The hypothesis is explore the relationship for predictors (babysex, bhead, blength, wtgain, smoken) and birthweight. The add the predictions and residual to explore the trend and make the point diagram to conclude it. Residuals form a horizontal (linear) ‘band’ around zero. However, there are some potential outliers in the lower range of birthweight on the left.


#### Compare other two models...
```{r}
#One using length at birth and gestational age as predictors (main effects only)
model2 <-lm(bwt ~ blength + gaweeks, data = birthweight)

#One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
model3<-lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data = birthweight)

cverror <- birthweight %>% 
  crossv_mc(100) %>% 
  mutate(train = map(train, as.tibble),
         test = map(test, as.tibble)) %>% 
  mutate(test_model1 = map(train, ~lm(bwt ~ babysex + bhead + blength + wtgain +smoken, data =.x)),
         effect_model2 =map(train, ~lm(bwt ~ blength + gaweeks, data=.x)),
         inter_model3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data=.x))) %>% 
  mutate(rmse_test_model1 = map2_dbl(test_model1,test, ~rmse(model = .x, data=.y)),
         rmse_effect_model2 = map2_dbl(effect_model2,test, ~rmse(model = .x, data=.y)),
         rmse_inter_model3 = map2_dbl(inter_model3,test, ~rmse(model = .x, data=.y))) %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model=str_replace(model, "rmse_", ""),
         model = fct_inorder(model))
         
```

```{r}
ggplot(cverror, aes(x=model, y = rmse, fill = model))+
  geom_violin()
```

In this plot, we can observe that the test_model1 has a much lower rmse that the other two, so test_model1 is might be a better model compared to the other two.

